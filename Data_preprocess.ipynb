{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Waveform Matched Subset Data Preparation for RAIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import tz\n",
    "import random\n",
    "import glob\n",
    "import csv\n",
    "import os.path\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CNN_duration = 30 # seconds\n",
    "L1RNN_num_states = 1*60 # 1 hour\n",
    "L1RNN_duration = L1RNN_num_states * CNN_duration # 1 hour\n",
    "L2RNN_num_states = 12 # 12 hours\n",
    "L2RNN_duration = L2RNN_num_states * L1RNN_duration # 0.5 day\n",
    "hz = 125\n",
    "\n",
    "shortest_length = 1.0 # hours\n",
    "waveform_grace_start = 0.5 # hours\n",
    "future_time_interval = 24.0 # For computing decompensation, i.e., mortaility in the next xx hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process per dir\n",
    "p_dir = 'p00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Construct benchmark dataset from clinical database using\n",
    "```\n",
    "https://github.com/YerevaNN/mimic3-benchmarks\n",
    "```\n",
    "\n",
    "## 0.2. Convert waveforms/numerics to .txt\n",
    "```\n",
    "\n",
    "rdsamp -r p00/p000020/p000020-2183-04-28-17-47 -p -v -s “II”\n",
    "rdsamp -r p00/p000020/p000020-2183-04-28-17-47n -p -v -s “ABPSys”\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "## 0.3. Align the benchmark clinical data with the waveform data\n",
    "\n",
    "```perl\n",
    "data_preprocess/align_icustay_episode_waveform.perl: \n",
    "\n",
    "#!/usr/local/bin/perl\n",
    "\n",
    "use Date::Parse;\n",
    "\n",
    "$waveform_record_file = \"MIMIC_data/waveform_matched_subset/RECORDS-waveforms\";\n",
    "\n",
    "$icustay_file = \"MIMIC_data/clinical_data/from_benchmarks/all_stays.csv\";\n",
    "\n",
    "$test_split_dir = \"MIMIC_data/clinical_data/from_benchmarks/test/\";\n",
    "\n",
    "$train_split_dir = \"MIMIC_data/clinical_data/from_benchmarks/train/\";\n",
    "\n",
    "$out_file = \"mimic_data/intermediate_data/ICUSTAYS.waveform_matched.csv\";\n",
    "\n",
    "%hashSPLIT = ();\n",
    "%numICUSTAY = ();\n",
    "%hashICUSTAY = ();\n",
    "\n",
    "print \"Reading split list...\\n\";\n",
    "open( $fh, \"-|\", \"find\", $test_split_dir, \"-type\", \"d\" );\n",
    "$line = <$fh>;\n",
    "while ($line = <$fh>) {\n",
    "        chomp $line;\n",
    "        $line =~ s/$test_split_dir//g;\n",
    "        $hashSPLIT{$line} = 'test';\n",
    "}\n",
    "close $fh;\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the list of waveforms that will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPLIT</th>\n",
       "      <th>EPISODE</th>\n",
       "      <th>RECORD_WAVEFORM_FILE</th>\n",
       "      <th>RECORD_WAVEFORM_START_TIME</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>LAST_CAREUNIT</th>\n",
       "      <th>DBSOURCE</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>...</th>\n",
       "      <th>DEATHTIME</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>AGE</th>\n",
       "      <th>MORTALITY_INUNIT</th>\n",
       "      <th>MORTALITY</th>\n",
       "      <th>MORTALITY_INHOSPITAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>1932_episode1_timeseries.csv</td>\n",
       "      <td>p00/p001932/p001932-2127-04-18-15-25</td>\n",
       "      <td>2127-04-18 15:25:00</td>\n",
       "      <td>1932</td>\n",
       "      <td>123386</td>\n",
       "      <td>216461</td>\n",
       "      <td>CSRU</td>\n",
       "      <td>carevue</td>\n",
       "      <td>2127-04-18 09:43:40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN/NOT SPECIFIED</td>\n",
       "      <td>MYOCARDIAL INFARCTION;UNSTABLE ANGINA\\CATH</td>\n",
       "      <td>F</td>\n",
       "      <td>2045-05-03 00:00:00</td>\n",
       "      <td>2136-10-25 00:00:00</td>\n",
       "      <td>82.01206938102486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SPLIT                       EPISODE                  RECORD_WAVEFORM_FILE  \\\n",
       "0  train  1932_episode1_timeseries.csv  p00/p001932/p001932-2127-04-18-15-25   \n",
       "\n",
       "  RECORD_WAVEFORM_START_TIME SUBJECT_ID HADM_ID ICUSTAY_ID LAST_CAREUNIT  \\\n",
       "0        2127-04-18 15:25:00       1932  123386     216461          CSRU   \n",
       "\n",
       "  DBSOURCE               INTIME         ...          DEATHTIME  \\\n",
       "0  carevue  2127-04-18 09:43:40         ...                      \n",
       "\n",
       "               ETHNICITY                                   DIAGNOSIS GENDER  \\\n",
       "0  UNKNOWN/NOT SPECIFIED  MYOCARDIAL INFARCTION;UNSTABLE ANGINA\\CATH      F   \n",
       "\n",
       "                   DOB                  DOD                AGE  \\\n",
       "0  2045-05-03 00:00:00  2136-10-25 00:00:00  82.01206938102486   \n",
       "\n",
       "  MORTALITY_INUNIT MORTALITY MORTALITY_INHOSPITAL  \n",
       "0                0         0                    0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_episode_waveform_file = \"MIMIC_data/intermediate_data/ICUSTAYS.waveform_matched.%s.csv\" % p_dir\n",
    "\n",
    "csvfile = open(map_episode_waveform_file, 'r')\n",
    "map_waveform = [row for row in csv.reader(csvfile, delimiter = ',')]\n",
    "map_waveform_table = pd.DataFrame(map_waveform,columns = \n",
    "                                  [\"SPLIT\",\"EPISODE\",\"RECORD_WAVEFORM_FILE\",\n",
    "                                   \"RECORD_WAVEFORM_START_TIME\",\"SUBJECT_ID\",\n",
    "                                   \"HADM_ID\",\"ICUSTAY_ID\",\"LAST_CAREUNIT\",\n",
    "                                   \"DBSOURCE\",\"INTIME\",\"OUTTIME\",\"LOS\",\"ADMITTIME\",\n",
    "                                   \"DISCHTIME\",\"DEATHTIME\",\"ETHNICITY\",\"DIAGNOSIS\",\n",
    "                                   \"GENDER\",\"DOB\",\"DOD\",\"AGE\",\"MORTALITY_INUNIT\",\n",
    "                                   \"MORTALITY\",\"MORTALITY_INHOSPITAL\"])\n",
    "\n",
    "map_waveform_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Read waveform file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "waveform_dir = 'MIMIC_data/waveform_data'\n",
    "waveform_file = \"%s/%s.txt\" % (waveform_dir, 'p00/p001932/p001932-2127-04-18-15-25')\n",
    "# load csv file\n",
    "df = pd.read_csv(waveform_file, delimiter = '\\t', header = None, \n",
    "                 names = ['time','II'],skipinitialspace=True, \n",
    "                 skiprows=2,dtype={'time':np.float64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Process waveform file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing MIMIC_data/waveform_data/p00/p001932/p001932-2127-04-18-15-25.txt...\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "sample_cnt=0\n",
    "out_dir = 'MIMIC_data/out_data/pickle_data'\n",
    "with open(map_episode_waveform_file, \"rb\") as f:\n",
    "\trd = csv.reader(f, delimiter = ',')\n",
    "\t#rd.next()\n",
    "\tfor row in rd:\n",
    "\t\tsplit = row[0]\n",
    "\t\tepisode = row[1]\n",
    "\t\twaveform = row[2]\n",
    "\t\tstart_time = row[3]\n",
    "\t\tsubject_id = row[4]\n",
    "\t\tintime = row[9]\n",
    "\t\touttime = row[10]\n",
    "\t\tlos = row[11]\n",
    "\t\tdeathtime = row[19] # use DOD\n",
    "\t\tmortality = row[22] \n",
    "\n",
    "\t\twaveform_file = \"%s/%s.txt\" % (waveform_dir, waveform)\n",
    "\t\tif not os.path.exists(waveform_file):\n",
    "\t\t\tprint(\"%s not exists!\" % waveform_file)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif los == '':\n",
    "\t\t\tprint(\"%s los not exists!\" % waveform_file)\n",
    "\t\t\tcontinue\n",
    "\t\tprint(\"processing %s...\" % waveform_file)\n",
    "\n",
    "\t\tlos = float(row[11]) * 24.0\t\n",
    "\t\n",
    "\t\tif deathtime == '':\n",
    "\t\t\tlived_time = 1e18\n",
    "\t\telse:\n",
    "\t\t\tlived_time = (datetime.strptime(deathtime, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds() / 3600.0\n",
    "\t\t\n",
    "\t\twaveform_offset = (datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(intime, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "\t\tactual_start_offset = max(shortest_length * 3600 - waveform_offset, waveform_grace_start * 3600) \n",
    "\t\tlos = los - (actual_start_offset/3600.0)\n",
    "\t\tlived_time = lived_time -(actual_start_offset/3600.0)\n",
    "\n",
    "\t\tactual_start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\") + timedelta(seconds=actual_start_offset)\n",
    "\n",
    "\t\t# load csv file\n",
    "\t\tdf = pd.read_csv(waveform_file, delimiter = '\\t', header = None, names = ['time','II'],skipinitialspace=True, skiprows=2+int(actual_start_offset*hz),dtype={'time':np.float64})\t\n",
    "\t\t#csv_chunks = pd.read_csv(waveform_file, delimiter = '\\t', header = None, names = ['time','II'],skipinitialspace=True, skiprows=2+int(actual_start_offset*hz),chunksize = 10000)\t\n",
    "\t\t#df = pd.concat(chunk for chunk in csv_chunks)\n",
    "\t\tlen_df = len(df)\n",
    "\t\tlen_hr_df = len_df/(hz * 3600.0)\n",
    "\t\t\n",
    "\t\t# check empty files\n",
    "\t\tif len_df == 0:\n",
    "\t\t\tprint(\"Error! empty file\")\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# check empty II signal\n",
    "\t\tif df['II'].dtype == 'object':\n",
    "\t\t\tempty_loc = df.index[df['II'] == '-'].tolist()\n",
    "\t\t\tif len(empty_loc) > int(len_df/2):\n",
    "\t\t\t\tprint(\"Error! half of the II signal missing\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tfor loc in empty_loc:\n",
    "\t\t\t\tdf.at[loc,'II'] = 0.0\n",
    "\t\t\n",
    "\t\t# check sample hz\n",
    "\t\tlast_record_time = df['time'][len_df-1]/3600.0\n",
    "\t\tif int(last_record_time) > int(len_hr_df) + 6:\n",
    "\t\t\tprint(\"Skipped! %s has total record time %d vs. expected record time %d\" %( waveform_file, int(last_record_time), int(len_hr_df)))\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# data quality check\n",
    "\t\tnum_episodes = int(len_hr_df/L2RNN_num_states)\t\n",
    "\t\tif num_episodes == 0:\n",
    "\t\t\tprint(\"Skipped! %s has less than %d hours ECG records\" % (waveform_file, L2RNN_num_states))\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# procss the file\n",
    "\t\t#print(\"In total %d episodes\" % num_episodes)\n",
    "\t\tlenEcgSeg = CNN_duration * hz\n",
    "\n",
    "\t\tindex = 0\n",
    "\t\tfor ep in range(num_episodes):\n",
    "\t\t\tfinal_x = []\n",
    "\t\t\tfinal_los = []\n",
    "\t\t\tfinal_decompensation = []\n",
    "\t\t\tfinal_los_bucket = []\n",
    "\n",
    "\t\t\tcur_start_time = actual_start_time + timedelta(seconds=ep * L2RNN_duration)\n",
    "\t\t\tout_file = \"%s/%s/%s-%s.pickle\" % (out_dir, split, episode.replace(\"_timeseries.csv\",\"\"), cur_start_time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "\n",
    "\t\t\tfor i in range(L2RNN_num_states):\n",
    "\t\t\t\tHourEcgSeg = []\n",
    "\t\t\t\tstart_index = index + i * L1RNN_duration * hz\n",
    "\t\t\t\tfor j in range(L1RNN_num_states):\n",
    "\t\t\t\t\tstart_in_index = start_index + j * lenEcgSeg\n",
    "\t\t\t\t\tEcgSeg = np.array(df['II'][start_in_index:(start_in_index + lenEcgSeg)])\n",
    "\t\t\t\t\tHourEcgSeg.append(EcgSeg.reshape(lenEcgSeg,1))\n",
    "\t\t\t\tfinal_x.append(HourEcgSeg)\n",
    "\n",
    "\t\t\t\tcur_hrs = ep*L2RNN_num_states + i\n",
    "\t\t\t\tfinal_los.append(los - cur_hrs)\n",
    "\t\t\t\tcur_bucket = int((los-cur_hrs)/24)\n",
    "\t\t\t\tif cur_bucket > 8:\n",
    "\t\t\t\t\tcur_bucket = 8\n",
    "\t\t\t\tif cur_bucket >= 14:\n",
    "\t\t\t\t\tcur_bucket = 9\n",
    "\t\t\t\tfinal_los_bucket.append(cur_bucket)\n",
    "\n",
    "\t\t\t\tif mortality == '0':\n",
    "\t\t\t\t\tcur_mortality = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcur_mortality = int(lived_time - cur_hrs < future_time_interval)\n",
    "\t\t\t\tif cur_mortality == 1:\n",
    "\t\t\t\t\tprint(\"%s start decompensating at hour %d\" % (waveform_file, cur_hrs))\n",
    "\t\t\t\tfinal_decompensation.append(cur_mortality)\n",
    "\n",
    "\t\t\tindex = index + L2RNN_duration * hz\n",
    "\n",
    "\t\t\tfinal_x = np.array(final_x)\n",
    "\t\t\tfinal_los = np.array(final_los)\n",
    "\t\t\tfinal_decompensation = np.array(final_decompensation)\n",
    "\t\t\tfinal_los_bucket = np.array(final_los_bucket)\n",
    "\t\t\t#print(final_x.shape)\n",
    "\n",
    "\t\t\toutput = {'x_ecg':final_x, 'y_los':final_los, 'y_decompensation':final_decompensation, 'y_los_bucket':final_los_bucket}\n",
    "\t\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\t\t\tprint(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process numerics/vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing p00/p001932/p001932-2127-04-18-15-25...\n",
      "MIMIC_data/waveform_data/p00/p001932/p001932-2127-04-18-15-25n.ABP_Sys.txt not exists!\n",
      "MIMIC_data/waveform_data/p00/p001932/p001932-2127-04-18-15-25n.ABP_Dias.txt not exists!\n",
      "MIMIC_data/waveform_data/p00/p001932/p001932-2127-04-18-15-25n.NBP_Sys.txt not exists!\n",
      "MIMIC_data/waveform_data/p00/p001932/p001932-2127-04-18-15-25n.NBP_Dias.txt not exists!\n",
      "In total 7 episodes\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "out_dir = 'MIMIC_data/out_data/vital_pickle_data'\n",
    "\n",
    "signals = [\"ABPSys\",\"ABPDias\",\"ABP Sys\", \"ABP Dias\",\"NBPSys\", \"NBPDias\",\"NBP Sys\",\"NBP Dias\",\"PULSE\",\"RESP\",\"SpO2\"]\n",
    "signal_names = [\"ABPSys\",\"ABPDias\",\"NBPSys\", \"NBPDias\",\"PULSE\",\"RESP\",\"SpO2\"]\n",
    "\n",
    "freq = 60 # seconds\n",
    "\n",
    "sample_cnt=0\n",
    "with open(map_episode_waveform_file, \"rb\") as f:\n",
    "\trd = csv.reader(f, delimiter = ',')\n",
    "\t#rd.next()\n",
    "\tfor row in rd:\n",
    "\t\tsplit = row[0]\n",
    "\t\tepisode = row[1]\n",
    "\t\twaveform = row[2]\n",
    "\t\tstart_time = row[3]\n",
    "\t\tsubject_id = row[4]\n",
    "\t\tintime = row[9]\n",
    "\t\touttime = row[10]\n",
    "\t\tlos = row[11]\n",
    "\t\tdeathtime = row[14]\n",
    "\t\tmortality = row[22]\n",
    "\n",
    "\t\tif los == '':\n",
    "\t\t\tprint(\"%s los not exists!\" % waveform_file)\n",
    "\t\t\tcontinue\n",
    "\t\tlos = float(row[11]) * 24.0\t\n",
    "\t\t\n",
    "\t\tprint(\"processing %s...\" % waveform)\n",
    "\t\tnonempty_sigs = 0\n",
    "\t\tfor s in range(len(signals)):\n",
    "\t\t\tsig = signals[s]\n",
    "\t\t\twaveform_file = \"%s/%sn.%s.txt\" % (waveform_dir, waveform, sig.replace(\" \",\"_\"))\n",
    "\t\t\tif not os.path.exists(waveform_file):\n",
    "\t\t\t\tprint(\"%s not exists!\" % waveform_file)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsig_name = sig.replace(\" \",\"\")\n",
    "\t\t\tif nonempty_sigs == 0:\n",
    "\t\t\t\tdf = pd.read_csv(waveform_file, delimiter = '\\t', header = None, names = ['time',sig_name],skipinitialspace=True,skiprows = 2, dtype={'time':np.int32})\n",
    "\t\t\t\tlen_df = len(df)\n",
    "\t\t\t\tif len_df == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif df[sig_name].dtype == 'object':\n",
    "\t\t\t\t\tempty_loc = df.index[df[sig_name] == '-'].tolist()\n",
    "\t\t\t\t\tif len(empty_loc) > int(len_df/2):\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tfor loc in empty_loc:\n",
    "\t\t\t\t\t\tdf.at[loc,sig_name] = 0.0\n",
    "\t\t\t\tdf = df.loc[df['time'] % 60 == 0]\n",
    "\t\t\t\tdf = df.reset_index(drop=True)\n",
    "\t\t\t\tnonempty_sigs = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tsub_df = pd.read_csv(waveform_file, delimiter = '\\t', header = None, names = ['time',sig.replace(\" \",\"\")],skipinitialspace=True, skiprows = 2, dtype={'time':np.int32})\n",
    "\t\t\t\tlen_sub_df = len(sub_df)\n",
    "\t\t\t\tif len_sub_df == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif sub_df[sig_name].dtype == 'object':\n",
    "\t\t\t\t\tempty_loc = sub_df.index[sub_df[sig_name] == '-'].tolist()\n",
    "\t\t\t\t\tif len(empty_loc) > int(len_sub_df/2):\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\tfor loc in empty_loc:\n",
    "\t\t\t\t\t\tsub_df.at[loc,sig_name] = 0.0\n",
    "\t\t\t\tdf = df.join(sub_df.set_index('time'), on = 'time',how ='left')\n",
    "\n",
    "\t\tif nonempty_sigs == 0:\n",
    "\t\t\tprint(\"Error! empty file\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif deathtime == '':\n",
    "\t\t\tlived_time = 1e18\n",
    "\t\telse:\n",
    "\t\t\tlived_time = (datetime.strptime(deathtime, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds() / 3600.0\n",
    "\t\t\n",
    "\t\twaveform_offset = (datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(intime, \"%Y-%m-%d %H:%M:%S\")).total_seconds()\n",
    "\t\tactual_start_offset = max(shortest_length * 3600 - waveform_offset, waveform_grace_start * 3600) \n",
    "\t\tlos = los - (actual_start_offset/3600.0)\n",
    "\t\tlived_time = lived_time -(actual_start_offset/3600.0)\n",
    "\n",
    "\t\tactual_start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\") + timedelta(seconds=actual_start_offset)\n",
    "\n",
    "\t\tindex = int(actual_start_offset/freq)\n",
    "\t\tlen_df = len(df) - index\n",
    "\t\tlen_hr_df = len_df/60\n",
    "\t\t\n",
    "\t\t# data quality check\n",
    "\t\tnum_episodes = int(len_hr_df/L2RNN_num_states)\t\n",
    "\t\tif num_episodes == 0:\n",
    "\t\t\tprint(\"Skipped! less than %d hours vital sign records\" % (L2RNN_num_states))\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# procss the file\n",
    "\t\tprint(\"In total %d episodes\" % num_episodes)\n",
    "\n",
    "\t\tlenVitalSeg = 60 # 1 hour and 7 vital signs\n",
    "\t\tfor ep in range(num_episodes):\n",
    "\t\t\tfinal_x = []\n",
    "\t\t\tfinal_los = []\n",
    "\t\t\tfinal_decompensation = []\n",
    "\t\t\tfinal_los_bucket = []\n",
    "\n",
    "\t\t\tcur_start_time = actual_start_time + timedelta(seconds=ep * L2RNN_duration)\n",
    "\t\t\tout_file = \"%s/%s/%s-%sn.pickle\" % (out_dir, split, episode.replace(\"_timeseries.csv\",\"\"), cur_start_time.strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "\n",
    "\t\t\tfor i in range(L2RNN_num_states):\n",
    "\t\t\t\tHourVitalSeg = np.zeros(lenVitalSeg * 7) \n",
    "\t\t\t\tstart_index = index + i * lenVitalSeg \n",
    "\t\t\t\tfor s in range(len(signal_names)):\n",
    "\t\t\t\t\tsig_name = signal_names[s]\n",
    "\t\t\t\t\tif sig_name in df.columns:\t\n",
    "\t\t\t\t\t\tHourVitalSeg[(s * lenVitalSeg):((s+1) * lenVitalSeg)] = np.array(df[sig_name][start_index:(start_index+lenVitalSeg)])\n",
    "\t\t\t\tfinal_x.append(HourVitalSeg.reshape((lenVitalSeg*7),1))\n",
    "\n",
    "\t\t\t\tcur_hrs = ep*L2RNN_num_states + i\n",
    "\t\t\t\tfinal_los.append(los - cur_hrs)\n",
    "\t\t\t\tcur_bucket = int((los-cur_hrs)/24)\n",
    "\t\t\t\tif cur_bucket > 8:\n",
    "\t\t\t\t\tcur_bucket = 8\n",
    "\t\t\t\tif cur_bucket >= 14:\n",
    "\t\t\t\t\tcur_bucket = 9\n",
    "\t\t\t\tfinal_los_bucket.append(cur_bucket)\n",
    "\n",
    "\t\t\t\tif mortality == '0':\n",
    "\t\t\t\t\tcur_mortality = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcur_mortality = int(lived_time - cur_hrs < future_time_interval)\n",
    "\t\t\t\tfinal_decompensation.append(cur_mortality)\n",
    "\n",
    "\t\t\tindex = index + lenVitalSeg * L2RNN_num_states\n",
    "\n",
    "\t\t\tfinal_x = np.array(final_x)\n",
    "\t\t\tfinal_los = np.array(final_los)\n",
    "\t\t\tfinal_decompensation = np.array(final_decompensation)\n",
    "\t\t\tfinal_los_bucket = np.array(final_los_bucket)\n",
    "\n",
    "\t\t\toutput = {'x_vitals':final_x, 'y_los':final_los, 'y_decompensation':final_decompensation, 'y_los_bucket':final_los_bucket}\n",
    "\t\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\t\t\tprint(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Lab and demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinical_dir = 'MIMIC_data/clinical_data/from_benchmarks'\n",
    "demographic_features = [\"Ethnicity\",\"Gender\",\"Age\"]\n",
    "demographic_dtype = {\"Ethnicity\":np.int32,\"Gender\":np.int32,\"Age\":np.float64}\n",
    "temporal_features = [\"oxygen saturation\",\"diastolic blood pressure\",\n",
    "                     \"heart rate\",\"mean blood pressure\",\n",
    "                     \"respiratory rate\",\"systolic blood pressure\"]\n",
    "lab_features = [\"glucose\",\"ph\",\"temperature\"]\n",
    "series_dtype = {\"Hours\":np.float64,\"glucose\":np.float64,\n",
    "                \"oxygen saturation\":np.float64,\"ph\":np.float64,\n",
    "                \"temperature\":np.float64,\"diastolic blood pressure\":np.float64,\n",
    "                \"heart rate\":np.float64,\"mean blood pressure\":np.float64,\n",
    "                \"respiratory rate\":np.float64,\"systolic blood pressure\":np.float64}\n",
    "\n",
    "in_dir = 'MIMIC_data/out_data/pickle_data'\n",
    "out_dir = 'MIMIC_data/out_data/lab_demog_pickle_data'\n",
    "\n",
    "def getLabDemog(hashEpisode, clinical_dir, in_dir, out_dir, split):\n",
    "\tin_files = os.listdir(\"%s/%s\" % (in_dir,split))\n",
    "\tfor f in in_files:\n",
    "\t\tf_name = f.replace(\".pickle\",\"\")\n",
    "\t\ttokens = f_name.split(\"-\")\t\t\n",
    "\t\tepisode = tokens[0]\n",
    "\n",
    "\t\ttimeseries_file = \"%s/%s/%s_timeseries.csv\" % (clinical_dir, split, episode.replace(\"_\",\"/\"))\n",
    "\t\tepisode_file = \"%s/%s/%s.csv\" % (clinical_dir, split, episode.replace(\"_\",\"/\"))\n",
    "\t\tout_file = \"%s/%s/%s.pickle\" %(out_dir, split, f_name)\n",
    "\n",
    "\t\tstart_time = \"%s-%s-%s %s:%s:%s\" % (tokens[1],tokens[2], tokens[3], tokens[4], tokens[5], tokens[6])\n",
    "\t\tintime = hashEpisode[\"%s_timeseries.csv\" % episode]\t\n",
    "\t\tprint(\"processing %s...\" % f_name)\n",
    "\n",
    "\t\t# for output\n",
    "\t\tdemographics = np.zeros(len(demographic_features))\n",
    "\t\tnonempty_labs = 0\n",
    "\t\tnonempty_labs_indicator = np.zeros(L2RNN_num_states)\n",
    "\t\tfeatures = []\n",
    "\t\tlen_labs = len(lab_features)\n",
    "\t\tlen_temporal_features = len(temporal_features)\n",
    "\t\tlen_features = len_labs + 2 * len_temporal_features\n",
    "\t\thrFeature = np.zeros(len_features)\n",
    "\t\tfor n in range(L2RNN_num_states):\t\t\n",
    "\t\t\tfeatures.append(hrFeature.reshape(len_features,1))\n",
    "\t\tfeatures = np.array(features)\n",
    "\n",
    "\t\t# check file exists\n",
    "\t\t# read demographics\n",
    "\t\tif os.path.exists(episode_file):\n",
    "\t\t\td_df = pd.read_csv(episode_file, dtype = demographic_dtype)\n",
    "\t\t\tif len(d_df) > 0:\n",
    "\t\t\t\tfor d in range(len(demographic_features)):\n",
    "\t\t\t\t\tdemographics[d] = d_df[demographic_features[d]][0]\n",
    "\n",
    "\t\t# read labs\n",
    "\t\tif not os.path.exists(timeseries_file):\n",
    "\t\t\toutput = {'demographics':demographics, 'nonempty_labs':nonempty_labs, 'nonempty_lab_indicators':nonempty_labs_indicator, 'features':features}\n",
    "\t\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\t\t\tprint(\"No clinical data. Store empty data\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ts_df = pd.read_csv(timeseries_file, dtype = {'Hours':np.float64})\n",
    "\t\tstart_interval = (datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(intime, \"%Y-%m-%d %H:%M:%S\")).total_seconds() / 3600.0\n",
    "\t\ts_df = s_df.loc[(s_df['Hours'] >= start_interval) & (s_df['Hours'] < (start_interval + L2RNN_num_states))]\n",
    "\t\ts_df = s_df.reset_index(drop=True)\n",
    "\n",
    "\t\tif len(s_df) == 0:\n",
    "\t\t\toutput = {'demographics':demographics, 'nonempty_labs':nonempty_labs, 'nonempty_lab_indicators':nonempty_labs_indicator, 'features':features}\n",
    "\t\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\t\t\tprint(\"No clinical data. Store empty data\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tlast_temporal_features = np.zeros(len_temporal_features)\n",
    "\t\tfor index in range(len(s_df)):\n",
    "\t\t\tcur_hr = int(s_df['Hours'][index] - start_interval)\t\n",
    "\t\t\tfor l in range(len_labs):\n",
    "\t\t\t\tcur_lab = s_df[lab_features[l]][index]\n",
    "\t\t\t\tif isinstance(cur_lab, basestring):\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\"-\",\"\")\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\"cs\",\"\")\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\"CS\",\"\")\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\" \",\"\")\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\"u\",\"\")\n",
    "\t\t\t\t\tcur_lab = cur_lab.replace(\"/\",\".\")\n",
    "\t\t\t\t\t#if cur_lab in invalid_values:\n",
    "\t\t\t\t\t#\tcur_lab = \"0\"\t\n",
    "\t\t\t\ttry:\t\n",
    "\t\t\t\t\tcur_lab = float(cur_lab)\n",
    "\t\t\t\texcept ValueError:\n",
    "\t\t\t\t\tprint(\"xxxxx %s\" % cur_lab)\n",
    "\t\t\t\t\tcur_lab = 0.0\n",
    "\n",
    "\t\t\t\tif pd.isnull(cur_lab):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfeatures[cur_hr][l] = cur_lab \n",
    "\t\t\t\tnonempty_labs = 1\n",
    "\t\t\t\tnonempty_labs_indicator[cur_hr] = nonempty_labs_indicator[cur_hr] + 1\n",
    "\n",
    "\t\t\tfor t in range(len_temporal_features):\n",
    "\t\t\t\tcur_feature = s_df[temporal_features[t]][index]\n",
    "\t\t\t\t#print(cur_feature)\n",
    "\t\t\t\tif isinstance(cur_feature, basestring):\n",
    "\t\t\t\t\tcur_feature = cur_feature.replace(\"-\",\"\")\n",
    "\t\t\t\t\tcur_feature = cur_feature.replace(\"cs\",\"\")\n",
    "\t\t\t\t\tcur_feature = cur_feature.replace(\"CS\",\"\")\n",
    "\t\t\t\t\tcur_feature = cur_feature.replace(\" \",\"\")\n",
    "\t\t\t\t\t#if cur_feature in invalid_values:\n",
    "\t\t\t\t\t#\tcur_feature = '0'\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tcur_feature = float(cur_feature)\n",
    "\t\t\t\texcept ValueError:\n",
    "\t\t\t\t\tprint(\"xxxxx %s\" % cur_feature)\n",
    "\t\t\t\t\tcur_feature = 0.0\n",
    "\t\t\t\tif pd.isnull(cur_feature):\n",
    "\t\t\t\t\tif last_temporal_features[t] !=0:\n",
    "\t\t\t\t\t\tfeatures[cur_hr][len_labs + t] = last_temporal_features[t]\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfeatures[cur_hr][len_labs + len_temporal_features + t] = 1\n",
    "\t\t\t\tif last_temporal_features[t] == 0:\n",
    "\t\t\t\t\tfor h in range(cur_hr):\n",
    "\t\t\t\t\t\tfeatures[h][len_labs + t] = cur_feature\n",
    "\t\t\t\tfeatures[cur_hr][len_labs + t] = cur_feature\n",
    "\t\t\t\tlast_temporal_features[t] = cur_feature\n",
    "\n",
    "\t\toutput = {'demographics':demographics, 'nonempty_labs':nonempty_labs, 'nonempty_lab_indicators':nonempty_labs_indicator, 'features':features}\n",
    "\t\t#print(output)\n",
    "\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\t\tprint(\"Done\")\n",
    "\n",
    "hashEpisode = {}\n",
    "with open(map_episode_waveform_file, \"rb\") as f:\n",
    "\trd = csv.reader(f, delimiter = ',')\n",
    "\t#rd.next()\n",
    "\tfor row in rd:\n",
    "\t\tepisode = row[1]\n",
    "\t\tintime = row[9]\n",
    "\t\thashEpisode[episode] = intime\n",
    "\n",
    "getLabDemog(hashEpisode, clinical_dir, in_dir, out_dir, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinical_dir = 'MIMIC_data/clinical_data/from_benchmarks'\n",
    "in_dir = 'MIMIC_data/out_data/pickle_data'\n",
    "out_dir = 'MIMIC_data/out_data/intervention_pickle_data'\n",
    "files = ['PROCEDUREEVENTS_MV_unique.csv',\n",
    "         'INPUTEVENTS_MV_unique.csv',\n",
    "         'INPUTEVENTS_CV_unique.csv']\n",
    "\n",
    "def getOthers(files,clinical_dir, in_dir, out_dir, split):\n",
    "\tin_files = os.listdir(\"%s/%s\" % (in_dir,split))\n",
    "\tfor f in in_files:\n",
    "\t\tf_name = f.replace(\".pickle\",\"\")\n",
    "\t\ttokens = f_name.split(\"-\")\t\t\n",
    "\t\tepisode = tokens[0]\n",
    "\t\ttoks = episode.split(\"_\")\n",
    "\t\tsub_id = toks[0]\n",
    "\n",
    "\t\tout_file = \"%s/%s/%s.pickle\" %(out_dir, split, f_name)\n",
    "\t\tstart_time = \"%s-%s-%s %s:%s:%s\" % (tokens[1],tokens[2], tokens[3], tokens[4], tokens[5], tokens[6])\n",
    "\n",
    "\t\t# for output\n",
    "\t\tnonempty_interventions = 0\n",
    "\t\tnonempty_interventions_indicator = np.zeros(L2RNN_num_states)\t\n",
    "\t\tprint(\"processing %s...\" % f_name)\n",
    "\t\tfor ff in files:\n",
    "\t\t\tinput_file = \"%s/%s/%s/%s\" % (clinical_dir, split, sub_id, ff)\n",
    "\t\t\tif not os.path.exists(input_file):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdf = pd.read_csv(input_file)\n",
    "\t\t\tif len(df) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tfor index in range(len(df)):\n",
    "\t\t\t\tcur_time = df['STARTTIME'][index]\n",
    "\t\t\t\tcur_hr = (datetime.strptime(cur_time, \"%Y-%m-%d %H:%M:%S\") - datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")).total_seconds() / 3600.0\t\t\t\t\n",
    "\t\t\t\tcur_hr = int(cur_hr)\n",
    "\t\t\t\tif cur_hr in range(L2RNN_num_states):\n",
    "\t\t\t\t\tnonempty_interventions_indicator[cur_hr] = nonempty_interventions_indicator[cur_hr] + 1\t\n",
    "\t\t\t\t\tnonempty_interventions = 1\n",
    "\n",
    "\t\toutput = {'nonempty_interventions':nonempty_interventions, 'nonempty_interventions_indicators':nonempty_interventions_indicator}\n",
    "\t\t#print(output)\n",
    "\t\twith open(out_file, 'wb') as o:\n",
    "\t\t\tpickle.dump(output, o, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "getOthers(files,clinical_dir, in_dir, out_dir, \"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
